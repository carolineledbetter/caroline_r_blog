---
title: A Tidymodels Workflow
author: Caroline Ledbetter
date: '2020-02-17'
publishDate: '2020-02-17'
slug: tidymodels-suite-demo
categories:
  - R
tags:
  - tidyverse
  - tidymodels
  - machine learning
lastmod: '2020-02-15T19:54:17-07:00'
description: ''
show_in_homepage: yes
show_description: 'Using the tidymodels package from beginning to end'
license: ''
featured_image: ''
featured_image_preview: ''
comment: yes
math: no
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

```{r, include = FALSE}
fct_dummy <- function(data, 
                      variables = tidyselect::everything(), 
                      sep = '.') {
  variables <- rlang::enquo(variables)
  # transform to long format the dummy columns
  tmp <- 
    tidyr::pivot_longer(data, 
                        cols = intersect(tidyselect::contains(sep),  
                                         !!variables),
                        names_to = c("groups", "levels"),
                        names_pattern = paste0("^([^'", sep, "]*)[", 
                                               sep, "](.*)"))
  
  # get the groups name for column selection after
  groups <- unique(tmp$groups)
  
  
  # keep only non dummy value and do not keep temp value col
  tmp <- dplyr::select(
    dplyr::filter(tmp, value == 1),
    -value)
  
  # function to return 'multiple' if more than 1 value is present
  ret_multiple <- function(x){
    if(length(x) > 1) return('multiple')
    return(x)
  }
  
  
  # tranform to wide format   
  tmp <- tidyr::pivot_wider(
    tmp,
    names_from = groups, 
    values_from = levels, 
    values_fn = list(levels = ret_multiple))
  
  
  # convert to factors the groups column
  dplyr::mutate_at(
    tmp,
    groups,
    ~ forcats::as_factor(.)
  )
}
```

```{r echo=FALSE}
blogdown::shortcode('tweet', '1204918320346157056')
```

I have been trying to incorporate more of the `tidymodels` suite of packages 
into my predictive models workflow (`rsample`, `recipes`) but I find myself 
frequently falling back on `caret` for model tuning and fitting 
because it's what I know. This post is a work through from start to finish using 
the `tidymodels` suite. This post is NOT a tutorial for supervised learning. 
It assumes you know how and why to split your data, resample, up/down sample,  
tune parameters, evaluate models etc. 
If you are looking for guides for machine learning, I highly recommend:  
:star: 
[Learning to teach machines to learn:](https://alison.rbind.io/post/2019-12-23-learning-to-teach-machines-to-learn/) 
This post from Alison Hill is full of great resources.  
:closed_book:[Applied Predictive Modeling](http://appliedpredictivemodeling.com) 
by Max Kuhn and Kjell Johnson  
:closed_book:
[Intro to Statisitcal Learning](https://www.springer.com/gp/book/9781461471370)
by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani  
:closed_book:
[The Elements of Statistical Learning](https://www.springer.com/gp/book/9780387848570)
by Trevor Hastie, Robert Tibshirani and Jerome Friedman  
----  
**Let's get started:**  
From the tidymodels readme:  

> tidymodels is a "meta-package" for modeling and statistical analysis
> that share the underlying design philosophy, grammar, and data structures
> of the tidyverse

**The Data**  
For this post I am going to use the 
[German Credit data](http://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)) 
from the University of California Irving Machine Learning Repository. This 
data set is included in the caret package, but the categorical variables are 
dummy coded and I want to demonstrate some of the factor options in recipe, so 
I am converting the dummy variables[^1]. 

```{r, message = FALSE}
library(tidymodels)
library(tune)
data(GermanCredit, package = "caret") 
german_credit <- 
  GermanCredit %>% 
  fct_dummy()
glimpse(german_credit)
```

```{r, include = FALSE}
rm(fct_dummy, GermanCredit)
```

**Training Data and Testing Data using rsample**  
The first thing I want to do is split my data into a training and testing set.
`rsample` has a function `initial_split`
that allows us to specify the proportion to be used for 
training (the default is 0.75) and a strata for stratified sampling (this allows 
us to ensure relative balance of our outcome between training and testing). 
A seed should always be set to ensure reproduciblity. 
`training` and `testing` functions then allow us to access the respective
data. 

```{r}
set.seed(1450)
credit_split <- german_credit %>% 
  initial_split(prop = 0.75, strata = Class)
credit_split

get_prop <- function(data, variable){
  data %>% 
    count({{variable}}) %>% 
    mutate(pct = n/sum(n))
}
map_dfr(list(training = training(credit_split), 
             testing = testing(credit_split)), 
    get_prop, variable = Class, 
    .id = 'source')
```

If we don't set use stratified sampling for the outcome, we are likely to have 
some imbalance between training and testing as seem below. 

```{r}
set.seed(1450)
credit_split_no_strata <- german_credit %>% 
  initial_split(prop = 0.75)
credit_split_no_strata

map_dfr(list(training = training(credit_split_no_strata), 
             testing = testing(credit_split_no_strata)), 
    get_prop, variable = Class, 
    .id = 'source')
```

We will move forward with the training data from the stratified sampling. 

```{r}
training(credit_split) %>% glimpse
```

**Preprocessing and Feature Engineering Using recipes**  
Next we will preprocess the data using `recipes`. 

* Our recipe will need a 
training data set and a formula, minimially.  
* One thing we may want to do 
is to consider the ordinal nature of `EmploymentDuration`.  
    + First we need to 
   make sure that the factors are ordered correctly.  
    + Then we can use 
   `step_ordinalscore` to convert an ordinal factor to a numeric score.  
* We can all convert our strings to factors  
* and center and scale our numerics.  
* Additional things that can be done we will skip:
    + impute missing values
    + remove variables that are highly sparse and unbalanced
    + up or down sample unbalanced outcomes
    + filter the data using the same syntax as `filter`

There are many more steps available, the ref docs are 
[here.](https://tidymodels.github.io/recipes/reference/index.html) 

```{r}
our_recipe <- 
  training(credit_split) %>% 
  recipe(Class ~ .) %>%
  step_mutate(EmploymentDuration = factor(EmploymentDuration, 
                                          levels = c('Unemployed', 
                                                     'lt.1', 
                                                     '1.to.4', 
                                                     '4.to.7', 
                                                     'gt.7'), 
                                          ordered = TRUE)
              ) %>% 
  step_ordinalscore(EmploymentDuration) %>% 
  step_string2factor(all_nominal()) %>% 
  step_center(all_numeric()) %>% 
  step_scale(all_numeric()) %>% 
  # step_knnimpute(all_predictors()) %>% 
  # step_nzv(all_predictors()) %>% 
  # step_upsample(all_outcomes(), over_ratio = 1) %>% 
  # step_filter(age > 70) %>% 
  prep
  
our_recipe
```

Once our recipe is ready to go, it's time to juice!

```{r}
training_data <- juice(our_recipe) 
glimpse(training_data)
```

The next thing I want to do is setup cross-validation to tune model parameters
using my training data. We will go back to `rsample` for this. 

```{r}
(train_cv <- 
  training_data %>% 
  vfold_cv(v = 5, repeats = 3))
```
  

[^1]: Alternatively the original data can be downloaded 
[here]("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data")
but this way gives relatively tidy variable names and factors. The code for
converting to factors can be found in a 
[previous post.](https://carolineledbetter.us/2020/01/dummy-to-factor/) 


